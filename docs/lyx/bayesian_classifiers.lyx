#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section*
Bayesian Classifiers
\end_layout

\begin_layout Subsection*
Naive Bayes classifier
\end_layout

\begin_layout Standard
Naive Bayes classifiers are a family of simple probabilistic classifiers
 based on applying Bayes' theorem with 
\emph on
strong (naive) independence assumptions
\emph default
 between the features.
\end_layout

\begin_layout Subsubsection*
Probabilistic model
\end_layout

\begin_layout Standard
Abstractly, naive Bayes is a conditional probability model: given a problem
 instance to be classified, represented by a vector 
\begin_inset Formula $\bm{x}=\left(x_{1},\cdots,x_{n}\right)$
\end_inset

 representing some 
\begin_inset Formula $n$
\end_inset

 features (independent variables), it assigns to this instance probabilities
\begin_inset Formula 
\[
p\left(C_{k}|x_{1},\cdots,x_{n}\right)
\]

\end_inset

for each of k possible outcomes or classes 
\begin_inset Formula $C_{k}$
\end_inset

.
\end_layout

\begin_layout Standard
The problem with the above formulation is that if the number of features
 n is large or if a feature can take on a large number of values, then basing
 such a model on probability tables is infeasible.
 We therefore reformulate the model to make it more tractable.
 Using Bayes' theorem, the conditional probability can be decomposed as
\begin_inset Formula 
\[
p\left(C_{k}|x\right)=\frac{p\left(C_{k}\right)p\left(x|C_{k}\right)}{p\left(x\right)}
\]

\end_inset

Here 
\begin_inset Formula $p\left(x_{i}|x_{i+1},\cdots,x_{n},C_{k}\right)=p\left(x_{i}|C_{k}\right)$
\end_inset

 and the joint model can be written following the chain rules as:
\begin_inset Formula 
\begin{eqnarray*}
p\left(C_{k}|x_{1},\cdots,x_{n}\right) & = & p\left(x_{1}|x_{2},\cdots,x_{n},C_{k}\right)p\left(x_{2}|x_{3},\cdots,x_{n},C_{k}\right)\cdots p\left(x_{n}|C_{k}\right)p\left(C_{k}\right)\\
 & = & \frac{p\left(C_{k}\right)}{p\left(x\right)}\prod_{i=1}^{n}p\left(x_{i}|C_{k}\right)
\end{eqnarray*}

\end_inset

or
\begin_inset Formula 
\[
p\left(x|C_{k}\right)=\prod_{i=1}^{n}p\left(x_{i}|C_{k}\right)
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
Pros and Cons
\end_layout

\begin_layout Itemize
pros
\end_layout

\begin_deeper
\begin_layout Itemize
It is easy and fast to predict class of test data set.
 It also perform well in multi class prediction
\end_layout

\begin_layout Itemize
When assumption of independence holds, a Naive Bayes classifier performs
 better compare to other models like logistic regression and you need less
 training data.
\end_layout

\begin_layout Itemize
It perform well in case of categorical input variables compared to numerical
 variable(s).
 For numerical variable, normal distribution is assumed (bell curve, which
 is a strong assumption).
\end_layout

\end_deeper
\begin_layout Itemize
cons
\end_layout

\begin_deeper
\begin_layout Itemize
If categorical variable has a category (in test data set), which was not
 observed in training data set, then model will assign a 0 (zero) probability
 and will be unable to make a prediction.
 This is often known as “Zero Frequency”.
 To solve this, we can use the smoothing technique.
 One of the simplest smoothing techniques is called Laplace estimation.
\end_layout

\begin_layout Itemize
On the other side naive Bayes is also known as a bad estimator, so the probabili
ty outputs from predict_proba are not to be taken too seriously.
\end_layout

\begin_layout Itemize
Another limitation of Naive Bayes is the assumption of independent predictors.
 In real life, it is almost impossible that we get a set of predictors which
 are completely independent.
\end_layout

\end_deeper
\end_body
\end_document
